# K-NearestNeighbor 分类算法

## 概述

在模式识别领域中，K-近邻法（k Nearest Neighbor）是一种用于分类和回归的非参数统计方法。在这两种情况下，输入包含特征空间（Feature Space）中的k个最接近的训练样本。

- 在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若k = 1，则该对象的类别直接由最近的一个节点赋予。
- 在k-NN回归中，输出是该对象的属性值。该值是其k个最近邻居的值的平均值。

最近邻居法采用向量空间模型来分类，概念为相同类别的案例，彼此的相似度高，而可以借由计算与已知类别案例之相似度，来评估未知类别案例可能的分类。

K-NN是一种基于实例的学习，或者是局部近似和将所有计算推迟到分类之后的惰性学习。k-近邻算法是所有的机器学习算法中最简单的之一。

无论是分类还是回归，衡量邻居的权重都非常有用，使较近邻居的权重比较远邻居的权重大。例如，一种常见的加权方案是给每个邻居权重赋值为1/d，其中d是到邻居的距离。

邻居都取自一组已经正确分类（在回归的情况下，指属性值正确）的对象。虽然没要求明确的训练步骤，但这也可以当作是此算法的一个训练样本集。

k-近邻算法的缺点是对数据的局部结构非常敏感。

## 算法

K近邻算法的核心思想是，给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

训练样本是多维特征空间向量，其中每个训练样本带有一个类别标签。算法的训练阶段只包含存储的特征向量和训练样本的标签。

### 参数选择

在分类阶段，k是一个用户定义的常数。一个没有类别标签的向量（查询或测试点）将被归类为最接近该点的k个样本点中最频繁使用的一类。

如何选取一个最佳的K值取决于数据。

一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较小的K值会容易产生过拟合的现象，很容易将一些噪声学习到模型中，而忽略了数据真实的分布。一个较好的K值能通过各种启发式技术来获取。

在二元（两类）分类问题中，K值尽量要取奇数，以保证最后的计算结果会产生一个较多的类别，如果取偶数则可能会产生相等的情况，不利于预测。

噪声和非相关性特征的存在，或特征尺度与它们的重要性不一致会使K近邻算法的准确性严重降低。对于选取和缩放特征来改善分类已经作了很多研究。一个普遍的做法是利用进化算法优化功能扩展，还有一种较普遍的方法是利用训练样本的互信息进行选择特征。

常用的取K值方法是从k=1开始，使用检验集估计分类器的误差率，重复该过程，每次K增值1，允许增加一个近邻。选取产生误差最小率的K。一般K的取值不超过20，上限是n的开方，随着数据集的增大，K值也要增大。

### 距离的度量

在之前的A star 中介绍过几个距离的计算方式，这里最常见使用欧式距离，

$$
设特征空间X是n维实数向量空间R^n，x_i,x_j \in X,
$$
$$
x_i = (x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^\tau，x_j = (x_j^{(1)},x_j^{(2)},...,x_j^{(n)})^\tau， 
$$
$$
x_i,x_j的欧式距离定义为：
$$
$$
L_2(x_i,x_j) = \sqrt{\sum_{l=1}^{n}|x_i^{(l)} + x_j^{(l)}|^2}
$$

一般情况下，将欧氏距离作为距离度量，但是这是只适用于连续变量。在文本分类这种离散变量情况下，另一个度量——重叠度量（或海明距离）可以用来作为度量。例如对于基因表达微阵列数据，k-NN也与Pearson和Spearman相关系数结合起来使用。通常情况下，如果运用一些特殊的算法来计算度量的话，k近邻分类精度可显著提高，如运用大间隔最近邻居或者邻里成分分析法。

### 特征值分类参数

#### 加权最近邻分类器

“多数表决”分类会在类别分布偏斜时出现缺陷。也就是说，出现频率较多的样本将会主导测试点的预测结果，因为他们比较大可能出现在测试点的K邻域而测试点的属性又是通过k邻域内的样本计算出来的。解决这个缺点的方法之一是在进行分类时将样本到k个近邻点的距离考虑进去。k近邻点中每一个的分类（对于回归问题来说，是数值）都乘以与测试点之间距离的成反比的权重。另一种克服偏斜的方式是通过数据表示形式的抽象。例如，在自组织映射（SOM）中，每个节点是相似的点的一个集群的代表（中心），而与它们在原始训练数据的密度无关。K-NN可以应用到SOM中。

k-最近邻分类器可以被视为 为k最近邻分配权重 1/k ，为其他所有邻居分配权重 0，这也可以推广高加权最近邻分类器。也就是说，第i近的邻居被赋予权重$\omega_{ni}$，其中， 
$$
\sum_{i=1}^{n}\omega_{ni}
$$

#### 特征值归一化

当有多个维度作为特征值的选取时，需要进行特征值的归一化，以减少特征值本身值对计算结果的偏差量，防止计算结果偏向于特征量大的维度结果，从而导致预测错误。

这时候可以采用加权的特征值，对特征值进行标准化处理。

一般来说，假设进行kNN分类使用的样本特征是
$$
{(x_{i1},x_{i2},...,x_{in})_{i=1}^m}
$$
取每一轴上的最大值减最小值
$$
M_j = \max_{i=1,...,m}x_{ij} - \min_{i=1,...,m}x_{ij}
$$
并且在计算距离时进将每一个坐标轴除以相应的Mj以进行归一化，即：
$$
d((y_i,...,y_n),(z_1,...,z_n)) = \sqrt{\sum_{j=1}^n(\frac{y_j}{M_j} - \frac{z_j}{M_j})^2}
$$

## 总结

K近邻算法是最简单有效的分类算法，简单且容易实现。但是当训练数据集很大时，需要占用大量的存储空间，而且需要计算的量比较大，时间复杂度为O(n)，当数据量较大时，可以将数据以树的形式呈现，能提高速度。

适用于类内间距小，类间间距大的数据集，并且对于类间边界分布不规则的数据效果要好于线性的分类器。

